from evaluation import Evaluation
import func_timeout
from collections import Counter
from call_llm import LLM


class ProgramOfThoughts(Evaluation):
    """
    POT: Program of Thoughts (https://github.com/TIGER-AI-Lab/Program-of-Thoughts/tree/main)
    Idea: Let llm translate the question into Python code, and then run the code locally to get the answer.
    """

    def __init__(self, llm, record_path, num_of_shots=0, num_of_trials=1):
        super().__init__(llm, record_path)
        # zero-shot or few-shot
        self.num_of_shots = num_of_shots
        # decide whether to use greedy or self-consistency, default greedy
        self.num_of_trials = num_of_trials
        self.prompt_path = 'prompt/pot_prompt_original.txt'
        self.system_prompt = """Your task is to solve math word problems using Python code.
Assign the final result to a variable called `ans`.
Provide only runnable Python code."""

    def get_prompt_list(self):
        """
        read and transform the prompt text into a list of qa tuples
        """
        with open(self.prompt_path, 'r') as f:
            prompt_text = f.read()
            qa_list = prompt_text.split('\n\n')
            n_shots_list = []
            for qa in qa_list:
                qa = qa.split('# Python code, return ans')
                n_shots_list.append((qa[0].strip(), qa[1].strip()))
            return n_shots_list

    def question_prompt(self, question):
        return f'Question: {question}'

    def answer_prompt(self, answer):
        return f'# Python code, return ans\n{answer}'

    def n_shot_chats(self, n: int, question: str):
        chats = [
            {"role": "system",
             "content": self.system_prompt}
        ]

        for q, a in self.get_prompt_list()[:n]:
            chats.append(
                {"role": "user", "content": q})
            chats.append(
                {"role": "assistant", "content": self.answer_prompt(a)})

        chats.append({"role": "user", "content": self.question_prompt(question)})
        return chats

    def evaluation(self, data):
        """
        compare the result from llm with the correct answer
        record the evaluation result in a jsonl file
        """
        llm_answer, total_completion_tokens, total_time, all_generated, prompt = self.program_of_thought(data)
        # convert the answer into numerical form
        answer = self.convert_answer(data['answer'])
        print('question', data['question'])
        print('answer vs llm_answer', answer, llm_answer)
        self.record_evaluation(data['question'], prompt, all_generated, answer, llm_answer, total_completion_tokens,
                               total_time)
        return llm_answer == answer

    def program_of_thought(self, data):
        """
        program of thoughts with greedy / self-consistency
        """
        total_completion_tokens = 0
        total_time = 0.0
        all_generated = []
        result_counter = Counter()
        # number of trials default 1 -> greedy
        # multiple trials -> self-consistency
        prompt = self.generate_prompt(data['question'])
        for i in range(self.num_of_trials):
            # call llm
            full_response = self.llm.get_full_response(prompt)
            llm_answer = self.convert_pot_answer(full_response['answer'])
            total_completion_tokens += full_response['completion_tokens']
            total_time += full_response['time']
            all_generated.append(full_response['answer'])
            print(llm_answer)
            if llm_answer is not None:
                result_counter.update([llm_answer])
        # get a majority vote
        if len(result_counter) > 0:
            llm_answer = result_counter.most_common(1)[0][0]
        else:
            llm_answer = None
        return llm_answer, total_completion_tokens, total_time, all_generated, prompt

    def generate_prompt(self, question):
        return self.n_shot_chats(self.num_of_shots, question)

    def convert_pot_answer(self, answer):
        exec_result = self.safe_execute(answer)
        float_answer = self.floatify_ans(exec_result)
        return self.delete_extra_zero(float_answer)

    def safe_execute(self, code_string: str):
        """
        run the code generated by llm
        """

        def execute(x):
            try:
                exec(x)
                locals_ = locals()
                return locals_.get('ans', None)
            except Exception:
                return None

        try:
            ans = func_timeout.func_timeout(5, execute, args=(code_string,))
        except func_timeout.FunctionTimedOut:
            ans = None

        return ans

    def floatify_ans(self, ans):
        if ans is None:
            return None
        elif type(ans) == dict:
            ans = list(ans.values())[0]
        elif type(ans) == bool:
            ans = ans
        elif type(ans) in [list, tuple]:
            if not ans:
                return None
            else:
                try:
                    ans = float(ans[0])
                except Exception:
                    ans = str(ans[0])
        else:
            try:
                ans = float(ans)
            except Exception:
                ans = str(ans)
        return ans


if __name__ == '__main__':
    llm = LLM()
    pot = ProgramOfThoughts(llm, 'pot2.jsonl', 8)
    pot.run_evaluation()
